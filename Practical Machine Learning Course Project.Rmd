---
title: "Practical Machine Learning Course Project"
author: "Lucas"
date: "4/4/2017"
output: html_document
---

##Practical Machine Learning Project

### Background 
With modern technology, personal wearables such as Jawbone Up, Nike FuelBand, and Fitnit it is now possible to collect a large amount of about personal acctivity releatively inexpensively. In this project, we use data from accelerometers on the belt, forearm, arm and dumbbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways represented by class A, B, C, D and E. By applying machine learning algorith to the training data set, we will attemp to build a model and identifiy the class category of the 20 test subjects in the test dataset. 

We will begin by loading necessary packages. 
```{r echo=TRUE, message=FALSE}
#Load necessary packages
rm(list=ls())
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(knitr)
library(rattle)
library(ggplot2)
library(corrplot)
#library(rattle)
set.seed(20000)
```

### Loading and cleaning the data
Then we will downlaod the dataset from the source.
```{r echo=TRUE, message=FALSE}
# set working directory
setwd("/Users/LucasXin/Documents/R/Sandbox/Data")

# Download dataset files
if (!file.exists("training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "training.csv")
}
if (!file.exists("testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "testing.csv")
}

quiz <- read.csv("testing.csv", sep = ",", na.strings = c("", "NA"))
data <- read.csv("training.csv", sep = ",", na.strings = c("", "NA"))
```

With the downlaoded dataset, lets look at the datasets first. 
```{r echo=TRUE,message=FALSE}

# Lets look at the size of the test dataset. 
dim(quiz)

# Next we will look at the size of the training dataset. 
dim(data)

# Partition the training data set 
inTrain <- createDataPartition(data$classe, p=0.8, list=FALSE)
myTraining <- data[inTrain, ]
myTesting <- data[-inTrain, ]

# We will confirm the partition by looking at the size of the training data set.
dim(myTraining)
# Next we will look at the size of the testing dataset. 
dim(myTesting)

```

Next we will clean up the dataset before we use it. 
```{r echo=TRUE}
#Cleaning the dataset

#Remove NearZeroVariance variables
nzv <- nearZeroVar(myTraining)
myTraining <- myTraining[, -nzv]
myTesting  <- myTesting[, -nzv]
dim(myTraining)
dim(myTesting)
#We see that the number of variables has been reduced to 118. 
#Next, we remove variables that are mostly 'NA'
myTraining<-myTraining[,colSums(is.na(myTraining))<0.95*nrow(myTraining)]
myTesting<-myTesting[,colSums(is.na(myTesting))<0.95*nrow(myTesting)]
dim(myTraining)
dim(myTesting)

#We see that the number of variables has been reduced to 59. 
#Next, we remove the columns for identification purposes
myTraining <- myTraining[, -(1:5)]
myTesting  <- myTesting[, -(1:5)]
dim(myTraining)
dim(myTesting)

#Finally, we see that the number of th variables has been reduced to 54. 
#At this point,we consider the dataset claen denough for data analysis.  
```

### Exploratory Data analysis

With the reduced dataset, next we will do some correlational analysis 
to explore the relationships between variables with corrplot package. 
```{r echo=TRUE}
#Correlation Analysis
corMatrix <- cor(myTraining[, -54])

cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
    diag(p.mat) <- 0
    diag(lowCI.mat) <- diag(uppCI.mat) <- 1
    for(i in 1:(n-1)){
        for(j in (i+1):n){
            tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
            p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
            lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
            uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
        }
    }
    return(list(p.mat, lowCI.mat, uppCI.mat))
}

res1 <- cor.mtest(corMatrix,0.95)
res2 <- cor.mtest(corMatrix,0.99)

corrplot(corMatrix, type="lower",tl.cex=0.8,tl.col=rgb(0,0,0),p.mat = res1[[1]], sig.level=0.05,insig="blank")
# The graph above shows the 1-to-1 relationships between different factors with a significant level of 0.05. 
# The graph below shows the 1-to-1 relationships between different factors with a significant level of 0.01. 
corrplot(corMatrix, type="lower",tl.cex=0.8,tl.col=rgb(0,0,0),p.mat = res1[[1]], sig.level=0.01,insig="blank")
```

### Prediction Model Building
We will apply three methods, namely Decision Tree, Random Froest and Generalized Boosted Model, to model this regression with the training data set and the one with the highest accuracy, as illutrated by the confusion matrix following each method, when applied to the test dataset will be used for the quiz predictions.

#### Prediction with Decision Tree method 
```{r echo=TRUE}
#Predict with decision tree
set.seed(20000)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```

```{r echo=TRUE}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
```

```{r echo=TRUE}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
```

#### Prediction with Random Forest method 
```{r echo=TRUE}
#Prediction with Random Forest
set.seed(20000)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```

```{r echo=TRUE}
#Plot modelFit for the Random Forest Method
plot(modFitB1)
```

```{r echo=TRUE}
#Plot the confusion Matrix for Random Forest method
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
```

#### Prediction with Generalized Boosted Regression method 
```{r echo=TRUE}
#Prediction with Generalized Boosted Regression
set.seed(20000)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)
gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)
gbmFinMod1 <- gbmFit1$finalModel
```

```{r echo=TRUE}
#Make predictions on the test dataset
gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest
```

```{r echo=TRUE}
# plot matrix results
plot(gbmAccuracyTest$table, col = gbmAccuracyTest$byClass, 
     main = paste("GBM Confusion Matrix: Accuracy =", round(gbmAccuracyTest$overall['Accuracy'], 4)))
```

### Applying the Selected Model to the Test Data

#### Below at the accuracy of the 3 regression modelling methods above: 
```{r echo=FALSE}
Models<-c("Random Forest","Decision Tree","GBM")
Accuracy<-c(round(cmrf$overall['Accuracy'], 4), round(cmtree$overall['Accuracy'], 4), round(gbmAccuracyTest$overall['Accuracy'], 4))
Summary<-cbind(Models,Accuracy)
kable(Summary,align="c")
```

```{r echo=TRUE}
#Based on the accuracy results obtained, Random Forest is the best method to apply to the quiz dataset. Below are the results for the test data set. 
quizTest <- predict(modFitB1, newdata=quiz)
quizTest
```